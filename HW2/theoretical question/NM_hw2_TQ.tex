\documentclass[11pt, a4paper]{article}
\usepackage{amsmath, amssymb, titling}
\usepackage[margin=2.5cm]{geometry}
\usepackage[colorlinks=true, linkcolor=black, urlcolor=black, citecolor=black]{hyperref}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\usepackage{cancel}
\usepackage{fancyhdr, lastpage}
\usepackage{fourier-orns}
\usepackage{xcolor}
\usepackage{nomencl}
\makenomenclature
\usepackage{etoolbox}
\usepackage{sidecap}
\usepackage{adjustbox}
\usepackage{listings}
\usepackage{matlab-prettifier}
\usepackage[T1]{fontenc}

\sidecaptionvpos{figure}{c}
\setlength{\headheight}{18.2pt}
\setlength{\nomlabelwidth}{1.5cm}

\renewcommand\maketitlehooka{\null\mbox{}\vfill}
\renewcommand\maketitlehookd{\vfill\null}

\renewcommand{\headrule}{\vspace{-5pt}\hrulefill\raisebox{-2.1pt}{\quad\leafleft\decoone\leafright\quad}\hrulefill}
\newcommand{\parder}[2]{\frac{\partial {#1}}{\partial {#2}}}
% \renewcommand\nomgroup[1]{%
%   \item[\bfseries
%   \ifstrequal{#1}{F}{Far--Away Properties}{%
%   \ifstrequal{#1}{N}{Dimensionless Numbers}{%
%   \ifstrequal{#1}{M}{Matrices}{%
%   \ifstrequal{#1}{D}{Diagonals}{%
%   \ifstrequal{#1}{V}{Vectors}{%
%   \ifstrequal{#1}{P}{Dimensionless Average Properties}{}}}}}}
% ]}

\title{Numerical Methods in Aeronautical Engineering \\ HW2 - Theoretical Questions}
\author{Almog Dobrescu ID 214254252}

% \pagestyle{fancy}
\cfoot{Page \thepage\ of \pageref{LastPage}}

\begin{document}

\maketitle
\thispagestyle{empty}
\newpage

\pagenumbering{roman}
% \setcounter{page}{1}

\tableofcontents
\newpage

\pagestyle{fancy}
\pagenumbering{arabic}
\setcounter{page}{1}

\section{Q2}
\subsection{A}
We are asked to prove:
\begin{equation}
    \delta^2=\Delta-\nabla
\end{equation}
Where:
\begin{itemize}
    \item $\delta f=f_{\left(x+\frac{h}{2}\right)}-f_{\left(x-\frac{h}{2}\right)}$
    \item $\Delta f=f_{\left(x+h\right)}-f_{\left(x\right)}$
    \item $\nabla f=f_{\left(x\right)}-f_{\left(x-h\right)}$
\end{itemize}
\begin{equation}
    \begin{matrix}
        \begin{array}{rcl}
            \delta^2f & = & \delta\left(f_{\left(x+\frac{h}{2}\right)}-f_{\left(x-\frac{h}{2}\right)}\right) \\\\
            & = & \delta f_{\left(x+\frac{h}{2}\right)}-\delta f_{\left(x-\frac{h}{2}\right)} \\\\
            & = & f_{\left(x+h\right)}-f_{\left(x\right)}-f_{\left(x\right)}+f_{\left(x-h\right)} \\\\
            & = & f_{\left(x+h\right)}-2f_{\left(x\right)}+f_{\left(x-h\right)}
        \end{array} && \begin{array}{rcl}
            \Delta f-\nabla f & = & f_{\left(x+h\right)}-f_{\left(x\right)}-f_{\left(x\right)}+f_{\left(x-h\right)} \\\\ 
            & = & f_{\left(x+h\right)}-2f_{\left(x\right)}+f_{\left(x-h\right)} \\\\
            \ \\\\
            \ \\\\
        \end{array} \\
        & \Downarrow &
    \end{matrix}
\end{equation}
\begin{equation*}
    \delta^2=\Delta-\nabla\qquad\blacksquare
\end{equation*}

\subsection{B}
The next ODE is given:
\begin{equation}
    \parder{U}{t}=\parder{^2U}{x^2}
\end{equation}
The following finite differencing method is suggested::
\begin{equation}
    u_{i,j+1}=u_{i,j}+R\left(u_{i-1,j}-u_{i,j}-u_{i,j+1}+u_{i+1,j+1}\right)
\end{equation}
\begin{itemize}
    \item $\displaystyle R=\frac{\Delta t}{h^2}$
\end{itemize}
in order to solve the method explicitly we will isolate $u_{i,j+1}$ in the LHS:
\begin{equation}
    \begin{array}{rcl}
        \left(1+R\right)u_{i,j+1} & = & u_{i,j}+R\left(u_{i-1,j}-u_{i,j}+u_{i+1,j+1}\right) \\\\
        \displaystyle u_{i,j+1} & = & \displaystyle \frac{1}{1+R}u_{i,j}+\frac{R}{1+R}\left(u_{i-1,j}-u_{i,j}+u_{i+1,j+1}\right)
    \end{array}
\end{equation}
This step might look like not enough, however, if we solve in a Gauss-Sidle like method, from the end to the start, so at a specific \emph{i} we would already know $u_{i+1,j+1}$.

\subsection{C}
Let's use forward differencing for the time derivative:
\begin{equation}
    \parder{U}{t}=\frac{1}{\Delta t}\Delta_tu=\frac{1}{\Delta t}\left(u_{i,j+1}-u_{i,j}\right)
\end{equation}
and central differencing for the spacial derivative:
\begin{equation}
    \parder{^2U}{x^2}=\frac{1}{h^2}\delta_x^2u=\frac{1}{h^2}\left(\Delta_x-\nabla_x\right)u
\end{equation}
To achieve the desired scheme we will define the forward differencing at $j+1$ and the backward differencing at $j$:
\begin{equation}
    \parder{^2U}{x^2}=\frac{1}{h^2}\left(\left.\Delta_x\right|_{j+1}-\left.\nabla_x\right|_j\right)u
\end{equation}
substituting the derivative into the ODE, we get:
\begin{equation}
    \begin{array}{rcl}
        \displaystyle \frac{1}{\Delta t}\left(u_{i,j+1}-u_{i,j}\right) & = & \displaystyle \frac{1}{h^2}\left(\left.\Delta_x\right|_{j+1}-\left.\nabla_x\right|_j\right)u \\\\
        \displaystyle \left(u_{i,j+1}-u_{i,j}\right) & = & \displaystyle \frac{\Delta t}{h^2}\left(u_{i+1,j+1}-u_{i,j+1}-u_{i,j}+u_{i-1,j}\right) \\\\
        \displaystyle u_{i,j+1} & = & \displaystyle u_{i,j}+R\left(u_{i+1,j+1}-u_{i,j+1}-u_{i,j}+u_{i-1,j}\right)\qquad\blacksquare
    \end{array}
\end{equation}

\section{Q3}
\subsection{A}
The two dimensional heat equation is given by:
\begin{equation}
    \parder{u}{t}=\parder{^2u}{x^2}+\parder{^2u}{y^2}
\end{equation}
u is a function of \emph{x}, \emph{y}, \emph{t}, namely $u=u_{\left(x,y,t\right)}$. We will derive the equation of $u_{\left(x,y,t+\Delta t\right)}$ by expanding it into a Taylor series:
\begin{equation}
    \begin{array}{rcl}
        \displaystyle u_{\left(x,y,t+\Delta t\right)} & = & \displaystyle u_{\left(x,y,t\right)}+\Delta t\parder{u_{\left(x,y,t\right)}}{t}+\frac{\left(\Delta t\right)^2}{2!}\parder{^2u_{\left(x,y,t\right)}}{t^2} +\frac{\left(\Delta t\right)^3}{3!}\parder{^3u_{\left(x,y,t\right)}}{t^3}+\cdots \\\\
        \displaystyle u_{\left(x,y,t+\Delta t\right)} & = & \displaystyle \left(1+\Delta t\parder{}{t}+\frac{\left(\Delta t\right)^2}{2!}\parder{^2}{t^2} +\frac{\left(\Delta t\right)^3}{3!}\parder{^3}{t^3}+\cdots\right)u_{\left(x,y,t\right)}
    \end{array}
\end{equation}
From the PDE we get the following relation:
\begin{equation}
    \begin{matrix}
        \displaystyle \parder{}{t}=\parder{^2}{x^2}+\parder{^2}{y^2}
    \end{matrix}
\end{equation}
So, the Taylor expansion can be rewritten as:
\begin{equation}
    \displaystyle u_{\left(x,y,t+\Delta t\right)}=\displaystyle \left(1+\Delta t\left(\parder{^2}{x^2}+\parder{^2}{y^2}\right)+\frac{\left(\Delta t\right)^2}{2!}\left(\parder{^2}{x^2}+\parder{^2}{y^2}\right)^2 +\frac{\left(\Delta t\right)^3}{3!}\left(\parder{^2}{x^2}+\parder{^2}{y^2}\right)^3+\cdots\right)u_{\left(x,y,t\right)}
\end{equation}
We can identify the the Taylor series of an exponential:
\begin{equation}
    \displaystyle u_{\left(x,y,t+\Delta t\right)}=\exp{\left(\Delta t\left(\parder{^2}{x^2}+\parder{^2}{y^2}\right)\right)}u_{\left(x,y,t\right)}
    \label{eq: converting to exponential}
\end{equation}
The derivative can be substituted by using the following operators relation:
\begin{equation}
    \begin{matrix}
        \displaystyle \parder{}{x}=D_x=\frac{2}{h_x}\sinh^{-1}{\left(\frac{\delta_x}{2}\right)} && \displaystyle \parder{}{y}=D_y=\frac{2}{h_y}\sinh^{-1}{\left(\frac{\delta_y}{2}\right)} \\\\
        \displaystyle \parder{^2}{x^2}=D_x^2=\frac{4}{h_x^2}\left(\sinh^{-1}{\left(\frac{\delta_x}{2}\right)}\right)^2 && \displaystyle \parder{^2}{y^2}=D_y^2=\frac{4}{h_y^2}\left(\sinh^{-1}{\left(\frac{\delta_y}{2}\right)}\right)^2
    \end{matrix}
\end{equation}
The following equation is reached:
\begin{equation}
    \displaystyle u_{\left(x,y,t+\Delta t\right)}=\exp{\left[\frac{4\Delta t}{h_x^2}\left(\sinh^{-1}{\left(\frac{\delta_x}{2}\right)}\right)^2+\frac{4\Delta t}{h_y^2}\left(\sinh^{-1}{\left(\frac{\delta_y}{2}\right)}\right)^2\right]}u_{\left(x,y,t\right)}
\end{equation}
To further simplify, we will expand the hyperbolic sin into it's Taylor series:
\begin{equation}
    \displaystyle u_{\left(x,y,t+\Delta t\right)}=\exp{\left[\frac{4\Delta t}{h_x^2}\left(\frac{\delta_x}{2}-\frac{1}{3!}\left(\frac{\delta_x}{2}\right)^3+\cdots\right)^2+\frac{4\Delta t}{h_y^2}\left(\frac{\delta_y}{2}-\frac{1}{3!}\left(\frac{\delta_y}{2}\right)^3+\cdots\right)^2\right]}u_{\left(x,y,t\right)}
\end{equation}
Now let's expand the exponent:
\begin{equation}
    \begin{array}{rcl}
        \displaystyle u_{\left(x,y,t+\Delta t\right)} & = & \displaystyle \left[1+\frac{4\Delta t}{h_x^2}\left(\frac{\delta_x}{2}-\frac{1}{3!}\left(\frac{\delta_x}{2}\right)^3+\cdots\right)^2+\frac{4\Delta t}{h_y^2}\left(\frac{\delta_y}{2}-\frac{1}{3!}\left(\frac{\delta_y}{2}\right)^3+\cdots\right)^2\right. \\\\
        && \displaystyle +\left.\frac{1}{2!}\left(\frac{4\Delta t}{h_x^2}\left(\frac{\delta_x}{2}-\frac{1}{3!}\left(\frac{\delta_x}{2}\right)^3+\cdots\right)^2+\frac{4\Delta t}{h_y^2}\left(\frac{\delta_y}{2}-\frac{1}{3!}\left(\frac{\delta_y}{2}\right)^3+\cdots\right)^2\right)^2\right]u_{\left(x,y,t\right)}\quad\blacksquare
    \end{array}
\end{equation}

\subsection{B}
From the this infinite Taylor series we can derive a lot of approximations. For example we could take only the elements up to the order of $\delta_x^2$ or $\delta_y^2$: 
\begin{equation}
    \begin{array}{rcl}
        \displaystyle u_{\left(u,y,t+\Delta t\right)} & = & \displaystyle \left[1+\left(\frac{4\Delta t}{h_x^2}\frac{\delta_x^2}{4}+\frac{4\Delta t}{h_y^2}\frac{\delta_y^2}{4}\right)\right]u_{\left(x,y,t\right)} \\\\
        \displaystyle u_{i,j,k+1} & = & \displaystyle \left[1+\frac{\Delta t}{h_x^2}\delta_x^2+\frac{\Delta t}{h_y^2}\delta^2_y\right]u_{i,j,k} \\\\
        \displaystyle u_{i,j,k+1} & = & \displaystyle u_{i,j,k}+R_x\left(u_{i-1,j,k}-2u_{i,j,k}+u_{i+1,j,k}\right)+R_y\left(u_{i,j-1,k}-2u_{i,j,k}+u_{i,j+1k}\right)\quad\blacksquare
    \end{array}
\end{equation}
\begin{itemize}
    \item $\displaystyle R_x=\frac{\Delta t}{h_x^2}$
    \item $\displaystyle R_y=\frac{\Delta t}{h_y^2}$
\end{itemize}

\section{Q4}
The next equation is given:
\begin{equation}
    \parder{U}{t}=\parder{^2U}{x^2}
\end{equation}
With the following boundary and initial conditions:
\begin{equation}
    \begin{matrix}
        \begin{array}{rcl}
            \displaystyle U_{\left(0,t\right)} & = & 0 \\\\
            \displaystyle \parder{U_{\left(1,t\right)}}{x} & = & M
        \end{array} && U_{\left(x,0\right)}={U_0}_{\left(x\right)}
    \end{matrix}
\end{equation}
In order to solve this equation we will use forward differencing in time and central differencing in space:
\begin{equation}
    u_{i,j+1}=u_{i,j}+R\left(u_{i-1,j+1}-2u_{i,j+1}+u_{i+1,j+1}\right)
\end{equation}
By rearranging we get:
\begin{equation}
    \underbrace{\left(-R\right)}_{\alpha_{i,j}} u_{i-1,j+1}+\underbrace{\left(1+2R\right)}_{\beta_{i,j}} u_{i,j+1}+\underbrace{\left(-R\right)}_{\gamma_{i,j}} u_{i+1,j+1}=\underbrace{u_{i,j}}_{RHS_{i,j}}
\end{equation}
There is no problem to solve the equations for $i=1,2,\cdots N$ but in the equation at $i=N+1$ we don't know $i=N+2$. We will use the boundary condition on the derivative at $i+N+1$. We will use central differencing to write the derivative:
\begin{equation}
    \begin{array}{c}
        \displaystyle \parder{u_{N+1,j}}{x}=\frac{1}{2h}\left(u_{N+2,j}-u_{N,j}\right)=M \\\\
        \displaystyle u_{N+2,j}=2hM+u_{N,j}
    \end{array}
\end{equation}
In matrix form we get:
\begin{equation}
    \begin{pmatrix}
        \beta_1,j & \gamma_1,j & 0 & \cdots & \cdots & \cdots & 0 \\
        \alpha_2,j & \beta_2,j & \gamma_2,j & 0 & \cdots & \cdots & 0 \\
        0 & \ddots & \ddots & \ddots & 0 & \cdots & 0 \\
        0 & 0 & \alpha_i & \beta_i & \gamma_i & 0 & 0 \\
        0 & \cdots & 0 & \ddots & \ddots & \ddots & 0 \\
        0 & \cdots & \cdots & 0 & \alpha_{N,j} & \beta_{N,j} & \gamma_{N,j} \\
        0 & 0 & \cdots & \cdots & 0 & \alpha_{N+1,j}+\gamma_{N+1,j} &\beta_{N+1,j}
    \end{pmatrix}
    \begin{pmatrix}
        u_{1,j+1} \\
        u_{2,j+1} \\
        \cdots \\
        \cdots \\
        \cdots \\
        u_{N,j+1} \\
        u_{N+1,j+1}
    \end{pmatrix}
    =
    \begin{pmatrix}
        RHS_{1,j}-\alpha_{0,j}\cdot u_{0,j} \\
        RHS_{2,j} \\
        \cdots \\
        \cdots \\
        \cdots \\
        RHS_{N,j} \\
        RHS_{N+1,j}-\gamma_{N+1,j}\cdot2hM
    \end{pmatrix}
\end{equation}
Using Thomas algorithm we would get a direct result for the $u_j$.

\end{document}